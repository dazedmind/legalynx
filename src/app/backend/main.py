# ================================
# ULTRA-OPTIMIZED FASTAPI MAIN
# ================================

import os
import sys
import time
import asyncio
from typing import Optional, Dict, Any
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Form
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
from dotenv import load_dotenv
from contextlib import asynccontextmanager
import jwt
import httpx
from rag_pipeline.streaming_query_engine import create_streaming_engine
import json

# Load environment variables
load_dotenv()

# ================================
# NLTK INITIALIZATION (Required for BM25Retriever)
# ================================
try:
    import nltk
    # Download required NLTK data silently at startup
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    print("âœ… NLTK data initialized at startup")
except Exception as e:
    print(f"âš ï¸ NLTK initialization warning: {e}")

# Add backend directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import optimized system with fallback handling
try:
    from optimized_rag_system import (
        optimized_upload_workflow,
        instant_upload_workflow,
        BackgroundRAGTracker,
        model_manager,
        RuleBasedFileNamer,
        apply_ultra_fast_config
    )
    OPTIMIZED_SYSTEM_AVAILABLE = True
    print("âœ… Optimized system with instant upload available")
except ImportError as e:
    print(f"âš ï¸ Optimized system not available: {e}")
    OPTIMIZED_SYSTEM_AVAILABLE = False

# Import existing modules with fallback handling
try:
    from security.enhanced_security_middleware import EnhancedSecurityMiddleware
    SECURITY_AVAILABLE = True
except ImportError as e:
    try:
        from security.security_middleware import SimplifiedSecurityMiddleware as EnhancedSecurityMiddleware
        SECURITY_AVAILABLE = True
        print("âœ… Fallback security middleware loaded")
    except ImportError as e2:
        print(f"âš ï¸ Security middleware not available: {e}, {e2}")
        SECURITY_AVAILABLE = False

try:
    from utils.file_handler import get_next_sequential_number, validate_pdf_content
    from utils.docx_converter import convert_docx_to_pdf, validate_docx_file
    UTILS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Utils not available: {e}")
    UTILS_AVAILABLE = False

# ================================
# LIFESPAN MANAGER
# ================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    print("ðŸš€ Starting Ultra-Fast RAG Pipeline with GPT-5...")
    print("âš¡ Pre-warming models for maximum speed...")
    
    # Pre-warm the singleton model manager if available
    if OPTIMIZED_SYSTEM_AVAILABLE:
        try:
            await model_manager.get_embedding_manager()
            print("âœ… Models pre-warmed successfully")
        except Exception as e:
            print(f"âš ï¸ Model pre-warming failed: {e}")
            print("   Models will be initialized on first upload")
        
        # Apply ultra-fast configuration
        apply_ultra_fast_config()
        print("âœ… Ultra-fast configuration applied")
    else:
        print("âš ï¸ Running in fallback mode without optimized system")
    
    yield  # This is where the app runs
    
    # Shutdown (if you need any cleanup)
    print("ðŸ›‘ Shutting down RAG Pipeline...")

# Initialize FastAPI app
app = FastAPI(
    title="Ultra-Fast RAG Pipeline API", 
    version="2.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global state
class RAGSystemManager:
    """
    Manages RAG systems with proper user/session isolation and cleanup.
    """
    def __init__(self):
        self.systems = {}  # document_id -> rag_system
        self.user_sessions = {}  # user_id -> current_document_id  
        self.session_systems = {}  # session_id -> current_document_id (for anonymous users)
        self.last_upload_time = {}  # document_id -> timestamp
        
    def set_system(self, document_id: str, rag_system: Dict[str, Any], file_path: str, user_id: Optional[str] = None, session_id: Optional[str] = None):
        """Set RAG system for a specific document and user/session."""
        print(f"ðŸ“ Setting RAG system for document: {document_id}, user: {user_id}, session: {session_id}")
        
        self.systems[document_id] = {
            "rag_system": rag_system,
            "file_path": file_path,
            "created_at": time.time(),
            "user_id": user_id,
            "session_id": session_id
        }
        
        # Track current system per user/session
        if user_id:
            self.user_sessions[user_id] = document_id
        elif session_id:
            self.session_systems[session_id] = document_id
            
        self.last_upload_time[document_id] = time.time()
        
        # Clean up old systems (keep only last 5 per user)
        self._cleanup_old_systems(user_id, session_id)
    
    def get_current_system(self, user_id: Optional[str] = None, session_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """Get the current RAG system for a specific user/session."""
        current_document_id = None
        
        # Find current document for this user/session
        if user_id and user_id in self.user_sessions:
            current_document_id = self.user_sessions[user_id]
        elif session_id and session_id in self.session_systems:
            current_document_id = self.session_systems[session_id]
            
        if not current_document_id or current_document_id not in self.systems:
            return None
            
        return self.systems[current_document_id]["rag_system"]
    
    def get_current_file_path(self, user_id: Optional[str] = None, session_id: Optional[str] = None) -> Optional[str]:
        """Get the current file path for a specific user/session."""
        current_document_id = None
        
        if user_id and user_id in self.user_sessions:
            current_document_id = self.user_sessions[user_id]
        elif session_id and session_id in self.session_systems:
            current_document_id = self.session_systems[session_id]
            
        if not current_document_id or current_document_id not in self.systems:
            return None
            
        return self.systems[current_document_id]["file_path"]
    
    def clear_current_system(self, user_id: Optional[str] = None, session_id: Optional[str] = None):
        """Clear the current system for a specific user/session (for error handling)."""
        print(f"ðŸ—‘ï¸ Clearing current RAG system for user: {user_id}, session: {session_id}")
        
        current_document_id = None
        if user_id and user_id in self.user_sessions:
            current_document_id = self.user_sessions[user_id]
            del self.user_sessions[user_id]
        elif session_id and session_id in self.session_systems:
            current_document_id = self.session_systems[session_id]
            del self.session_systems[session_id]
            
        if current_document_id:
            self.systems.pop(current_document_id, None)
    
    def get_status(self, user_id: Optional[str] = None, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Get status for a specific user/session."""
        current_system = self.get_current_system(user_id, session_id)
        return {
            "has_current_system": current_system is not None,
            "total_systems": len(self.systems),
            "user_sessions": len(self.user_sessions),
            "anonymous_sessions": len(self.session_systems)
        }
    
    def reset_all(self):
        """Reset all systems (admin function)."""
        print("ðŸ—‘ï¸ Resetting all RAG systems")
        self.systems.clear()
        self.user_sessions.clear()
        self.session_systems.clear()
        self.last_upload_time.clear()

    def get_current_document_id(self, user_id: Optional[str] = None, session_id: Optional[str] = None) -> Optional[str]:
        """Get the current document ID for a specific user/session."""
        if user_id and user_id in self.user_sessions:
            return self.user_sessions[user_id]
        elif session_id and session_id in self.session_systems:
            return self.session_systems[session_id]
        return None

    def _cleanup_old_systems(self, user_id: Optional[str] = None, session_id: Optional[str] = None):
        """Keep only the 5 most recent systems per user/session."""
        if len(self.systems) <= 10:  # Don't cleanup if we have few systems
            return
            
        # Sort by creation time and keep only recent ones
        sorted_systems = sorted(
            self.systems.items(),
            key=lambda x: x[1]["created_at"],
            reverse=True
        )
        
        # Keep only the most recent systems, but ensure current user's system is preserved
        systems_to_keep = {}
        current_user_doc = self.user_sessions.get(user_id) if user_id else None
        current_session_doc = self.session_systems.get(session_id) if session_id else None
        
        # Always keep current user/session document
        for doc_id, system_data in sorted_systems[:15]:  # Keep top 15 most recent
            systems_to_keep[doc_id] = system_data
            
        # Ensure current documents are preserved
        if current_user_doc and current_user_doc in self.systems:
            systems_to_keep[current_user_doc] = self.systems[current_user_doc]
        if current_session_doc and current_session_doc in self.systems:
            systems_to_keep[current_session_doc] = self.systems[current_session_doc]
            
        # Update systems dict
        removed_count = len(self.systems) - len(systems_to_keep)
        if removed_count > 0:
            print(f"ðŸ—‘ï¸ Cleaned up {removed_count} old RAG systems")
            self.systems = systems_to_keep

# Replace global variables with manager
rag_manager = RAGSystemManager()

# Security middleware (conditional initialization)
if SECURITY_AVAILABLE:
    security = EnhancedSecurityMiddleware()
else:
    security = None

# ================================
# PYDANTIC MODELS
# ================================

class QueryRequest(BaseModel):
    query: str
    voice_mode: bool = False  # Whether response should be concise for voice interaction
    
class QueryResponse(BaseModel):
    query: str
    response: str
    source_count: int
    security_status: Optional[str] = None
    
class SystemStatus(BaseModel):
    status: str
    pdf_loaded: bool
    index_ready: bool
    pdf_name: Optional[str] = None
    model_cache_status: str

class UploadResponse(BaseModel):
    message: str
    filename: str
    original_filename: str
    document_count: int
    document_id: str
    security_status: Optional[str] = None
    file_type: str
    conversion_performed: bool = False
    naming_applied: Optional[str] = None
    user_settings_used: bool = False
    processing_time: float
    timing_breakdown: Dict[str, float]
    optimization_used: str

# ================================
# USER SETTINGS (SIMPLIFIED)
# ================================

def get_default_user_settings() -> dict:
    """Fallback default settings."""
    return {
        'file_naming_format': 'ORIGINAL',
        'file_naming_title': 'Document',
        'file_client_name': 'Client'
    }

async def get_user_settings(user_id: str, auth_token: str) -> dict:
    """
    Get real user settings from your frontend API.
    Replace the mock implementation with this.
    """
    try:
        # Use your frontend API URL
        frontend_api_url = f"{os.environ.get('NEXT_PUBLIC_APP_URL', 'http://localhost:3000')}/backend/api/user/settings"
        
        headers = {
            "Authorization": f"Bearer {auth_token}",
            "Content-Type": "application/json"
        }
        
        print(f"ðŸ“‹ Fetching user settings for user: {user_id}")
        
        async with httpx.AsyncClient() as client:
            response = await client.get(frontend_api_url, headers=headers)
            
            if response.status_code == 200:
                settings_data = response.json()
                print(f"âœ… User settings loaded: {settings_data}")
                
                # Convert frontend format to backend format
                return {
                    'file_naming_format': settings_data.get('fileNamingFormat', 'ORIGINAL'),
                    'file_naming_title': settings_data.get('fileNamingTitle', 'Document'),
                    'file_client_name': settings_data.get('fileClientName', 'Client')
                }
            else:
                print(f"âš ï¸ Failed to fetch user settings: {response.status_code}")
                return get_default_user_settings()
                
    except Exception as e:
        print(f"âŒ Error fetching user settings: {e}")
        return get_default_user_settings()

def convert_enum_to_naming_option(enum_value: str) -> str:
    """Convert database enum to backend naming option format."""
    enum_mapping = {
        'ORIGINAL': 'keep_original',
        'ADD_TIMESTAMP': 'add_timestamp', 
        'ADD_CLIENT_NAME': 'add_client_name'
    }
    return enum_mapping.get(enum_value, 'keep_original')

def extract_user_id_from_token(request: Request) -> tuple[Optional[str], Optional[str]]:
    """
    Extract user ID and token from the Authorization header.
    Verifies the JWT using HS256 and the configured secret when available.
    Returns (user_id, token) where either may be None if not present/valid.
    """
    auth_header = request.headers.get('Authorization')
    if not auth_header or not auth_header.startswith('Bearer '):
        return None, None

    token = auth_header.replace('Bearer ', '').strip()

    # Prefer JWT_SECRET, fallback to JWT_SECRET_KEY for compatibility
    jwt_secret = os.environ.get('JWT_SECRET') or os.environ.get('JWT_SECRET_KEY')
    jwt_algorithms = ["HS256"]

    try:
        if jwt_secret:
            decoded = jwt.decode(token, jwt_secret, algorithms=jwt_algorithms)
        else:
            # If no secret configured, do not fail hard; treat as anonymous
            print("âš ï¸ JWT secret not configured. Skipping verification.")
            decoded = jwt.decode(token, options={"verify_signature": False})

        user_id = decoded.get('userId') or decoded.get('sub') or decoded.get('id')
        return user_id, token
    except jwt.ExpiredSignatureError:
        print("âš ï¸ JWT token has expired")
        return None, None
    except jwt.InvalidTokenError as e:
        print(f"âš ï¸ Invalid JWT token: {e}")
        return None, None

def extract_session_id_from_request(request: Request) -> str:
    """Extract or generate session ID for anonymous users."""
    # Try to get session ID from headers first
    session_id = request.headers.get('X-Session-Id')
    if session_id:
        return session_id
        
    # Fallback to IP + User-Agent hash for anonymous sessions
    client_ip = request.client.host if request.client else 'unknown'
    user_agent = request.headers.get('User-Agent', 'unknown')
    
    # Create a session identifier (in production, use proper session management)
    import hashlib
    session_data = f"{client_ip}:{user_agent}:{int(time.time() / 3600)}"  # Changes every hour
    session_id = hashlib.md5(session_data.encode()).hexdigest()[:12]
    
    return f"anon_{session_id}"
# ================================
# API ENDPOINTS
# ================================

@app.get("/", response_model=Dict[str, str])
async def root():
    """Root endpoint."""
    return {"message": "Ultra-Fast RAG Pipeline API is running"}

@app.get("/status", response_model=SystemStatus)
async def get_status(request: Request):
    """Get system status with proper state management."""

    # Extract user/session identifiers
    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)
    
    manager_status = rag_manager.get_status(user_id=user_id, session_id=session_id)
    current_file_path = rag_manager.get_current_file_path(user_id=user_id, session_id=session_id)

    # Safe model cache status check
    model_cache_status = "cold"
    if OPTIMIZED_SYSTEM_AVAILABLE:
        try:
            model_cache_status = "warmed" if model_manager._is_initialized else "cold"
        except:
            model_cache_status = "unknown"
    
    return SystemStatus(
        status="running",
        pdf_loaded=current_file_path is not None,
        index_ready=manager_status["has_current_system"],
        pdf_name=os.path.basename(current_file_path) if current_file_path else None,
        model_cache_status=model_cache_status
    )


@app.post("/upload-pdf-ultra-fast", response_model=UploadResponse)
async def upload_document_ultra_fast(
    request: Request, 
    file: UploadFile = File(...),
    # FIXED: Accept form data parameters
    naming_option: str = Form('keep_original'),
    title: str = Form('Document'),
    client_name: str = Form('Client'),
    document_id: Optional[str] = Form(None)
):
    """
    ULTRA-FAST UPLOAD: Complete workflow optimized for speed.
    FIXED: Now properly handles user settings from frontend.
    """

    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)

    print(f"ðŸ”‘ Processing upload for user: {user_id}, session: {session_id}")

    if not OPTIMIZED_SYSTEM_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="RAG system not available. Please check server configuration and dependencies."
        )
    
    start_time = time.time()
    # ðŸ”¥ SIMPLIFIED: Use database cuid ID instead of temporary doc_ ID
    # The document_id will be provided by the frontend after database creation
    if not document_id:
        # Fallback only - frontend should provide the database ID
        document_id = f"doc_{int(time.time() * 1000)}_{user_id or session_id}"
    
    try:
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="No filename provided")
        
        file_content = await file.read()
        file_size = len(file_content)
        
        # FIXED: Generate unique document ID only if not provided in form
        if not document_id:
            timestamp = int(time.time())
            random_suffix = hash(file.filename + str(file_size) + str(timestamp)) % 10000
            document_id = f"doc_{timestamp}_{random_suffix}"
        
        print(f"ðŸ†” Using Document ID: {document_id}")
        print(f"ðŸ“„ Processing: {file.filename} ({file_size:,} bytes)")
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        supported_extensions = ['.pdf', '.docx', '.doc']
        
        if file_ext not in supported_extensions:
            raise HTTPException(
                status_code=400, 
                detail=f"Unsupported file type: {file_ext}. Supported: {', '.join(supported_extensions)}"
            )

        # Size validation
        if file_size > 50 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File size too large (max 50MB)")
        if file_size == 0:
            raise HTTPException(status_code=400, detail="File is empty")
        
        # ===== STEP 2: GET USER SETTINGS (FIXED) =====
        user_id, auth_token = extract_user_id_from_token(request)
        
        print(f"ðŸ“‹ FIXED User settings received:")
        print(f"   - naming_option: {naming_option}")
        print(f"   - title: {title}")
        print(f"   - client_name: {client_name}")
        print(f"   - user_id: {user_id}")
        
        # ===== STEP 3: HANDLE DOCX CONVERSION =====
        final_file_content = file_content
        original_filename = file.filename
        file_type = "pdf"
        conversion_performed = False
        
        if file_ext in ['.docx', '.doc'] and UTILS_AVAILABLE:
            file_type = "docx"
            try:
                print(f"ðŸ“„ DOCX file detected: {file.filename}")
                
                from utils.file_handler import convert_docx_to_pdf
                
                # Convert DOCX to PDF (bytes in, bytes out)
                pdf_content = convert_docx_to_pdf(file_content, file.filename)
                
                # Replace content with PDF
                final_file_content = pdf_content
                file_type = "pdf"
                conversion_performed = True
                
                # âœ… IMPORTANT: Change filename extension to .pdf for uniformity
                # This ensures DOCX files are always saved as PDF in the system
                base_name = os.path.splitext(file.filename)[0]
                original_filename = f"{base_name}.pdf"
                
                print(f"âœ… DOCX converted to PDF ({len(pdf_content):,} bytes)")
                print(f"ðŸ“ Filename changed: {file.filename} â†’ {original_filename}")
                    
            except HTTPException:
                raise
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ DOCX conversion failed: {error_msg}")
                raise HTTPException(
                    status_code=500, 
                    detail=f"DOCX conversion failed. Please try converting the file to PDF manually."
                )

        # ===== STEP 4: STREAMLINED UPLOAD (SINGLE STRAIGHT PROCESS) =====
        build_start = time.time()

        # Use streamlined workflow - completes LLM naming before chat ready
        # Note: original_filename is now .pdf for converted DOCX files
        result = await optimized_upload_workflow(
            file_content=final_file_content,
            original_filename=original_filename,  # âœ… Uses .pdf extension for converted DOCX
            naming_option=naming_option,
            document_id=document_id,
            rag_manager=rag_manager,
            user_id=user_id,
            session_id=session_id,
            user_title=title,
            user_client_name=client_name,
            counter=1
        )

        final_filename = result["filename"]
        page_count = result.get("page_count", 0)

        # Calculate total time
        total_time = time.time() - start_time

        # Security status
        security_status = "verified"

        return UploadResponse(
            message=f"Document ready for chat! Upload completed in {total_time:.2f}s",
            filename=final_filename,
            original_filename=file.filename,
            document_count=page_count,
            document_id=document_id,
            security_status=security_status,
            file_type=file_type,
            conversion_performed=conversion_performed,
            naming_applied=naming_option,
            user_settings_used=user_id is not None,
            processing_time=total_time,
            timing_breakdown=result.get("timing", {}),
            optimization_used="streamlined_single_process"
        )

    except HTTPException:
        if document_id:
            rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
        raise
    except Exception as e:
        if document_id:
            rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
            
        error_msg = str(e)
        print(f"âŒ Error in ultra-fast upload: {error_msg}")
        
        # Clean up temp files
        temp_files = [
            f"temp_{file.filename}",
            f"temp_{os.path.splitext(file.filename)[0]}.pdf"
        ]
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
        rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
        
        # Check if this is a legal validation error
        if "Legal validation failed" in error_msg:
            # Extract the detailed rejection reason from the error message
            rejection_reason = error_msg.replace("Legal validation failed: ", "")
            raise HTTPException(
                status_code=400, 
                detail=rejection_reason
            )
        else:
            # Generic error
            raise HTTPException(status_code=500, detail=f"Error processing document: {error_msg}")
        

@app.post("/query", response_model=QueryResponse)
async def query_document_secure(request: Request, query_request: QueryRequest):
    """Query the RAG system with enhanced error handling and proper isolation."""

    # Add timing debug for all queries
    query_start = time.time()
    print(f"ðŸš€ QUERY RECEIVED: '{query_request.query[:50]}...' at {query_start}")

    # Check if streaming is requested
    stream = request.query_params.get("stream", "false").lower() == "true"
    print(f"ðŸ” STREAMING REQUESTED: {stream}")

    if stream:
        print(f"ðŸ“¡ REDIRECTING TO STREAMING at {time.time() - query_start:.3f}s")
        return await stream_query_document(request, query_request)
    
    # ... existing code ...
    """Query the RAG system with enhanced error handling and proper isolation."""
    
    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)

    print(f"ðŸ” Query received: '{query_request.query}'")
    print(f"ðŸ” Query length: {len(query_request.query)} characters")

    # Check if document is uploaded but RAG still building
    current_doc_id = None
    if user_id and user_id in rag_manager.user_sessions:
        current_doc_id = rag_manager.user_sessions[user_id]
    elif session_id and session_id in rag_manager.session_systems:
        current_doc_id = rag_manager.session_systems[session_id]

    if current_doc_id and OPTIMIZED_SYSTEM_AVAILABLE:
        rag_status = BackgroundRAGTracker.get_status(current_doc_id)
        if rag_status.get("status") == "processing":
            stage = rag_status.get("stage", "initializing")
            elapsed = time.time() - rag_status.get("started_at", time.time())
            eta = max(5, 60 - elapsed)

            print(f"â³ Document {current_doc_id} still indexing: {stage}")
            raise HTTPException(
                status_code=202,
                detail=f"Your document is still being indexed (stage: {stage}). Please wait about {eta:.0f} more seconds."
            )

    # Step 1: Get current RAG system from manager
    rag_system = rag_manager.get_current_system(user_id=user_id, session_id=session_id)

    if not rag_system:
        print(f"âŒ No RAG system available for user: {user_id}, session: {session_id}")
        print(f"ðŸ“Š Manager status: {rag_manager.get_status(user_id, session_id)}")
        raise HTTPException(
            status_code=400,
            detail="No document has been uploaded and indexed yet. Please upload a document first."
        )
    
    current_doc_id = None
    if user_id and user_id in rag_manager.user_sessions:
        current_doc_id = rag_manager.user_sessions[user_id]
    elif session_id and session_id in rag_manager.session_systems:
        current_doc_id = rag_manager.session_systems[session_id]
    print(f"âœ… Using RAG system: {current_doc_id}")
    print(f"âœ… RAG system keys: {list(rag_system.keys())}")
    
    # Step 2: Check if query engine exists
    if "query_engine" not in rag_system:
        print("âŒ No query engine in RAG system")
        print(f"âŒ Available RAG system keys: {list(rag_system.keys())}")
        # Clear broken system
        rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
        raise HTTPException(
            status_code=500, 
            detail="Query engine not properly initialized. Please re-upload your document."
        )
    
    query_engine = rag_system["query_engine"]
    print(f"âœ… Query engine found: {type(query_engine)}")
    
    # Step 3: Extract user ID
    print(f"ðŸ‘¤ User ID: {user_id or 'anonymous'}")
    
    try:
        # Step 4: Enhanced security check and sanitization
        sanitized_query = query_request.query
        was_sanitized = False
        
        if security:
            try:
                print("ðŸ›¡ï¸ Running enhanced security check...")
                sanitized_query, query_analysis = security.check_query_security(user_id, query_request.query)
                was_sanitized = sanitized_query != query_request.query
                print(f"âœ… Security check passed. Sanitized: {was_sanitized}")
                if was_sanitized:
                    print(f"ðŸ”§ Original: {query_request.query}")
                    print(f"ðŸ”§ Sanitized: {sanitized_query}")

            except Exception as security_error:
                sanitized_query = query_request.query
                was_sanitized = False
        else:
            print("âš ï¸ Security middleware not available, proceeding without security check")
        
        # Step 5: Validate query content
        if not sanitized_query or not sanitized_query.strip():
            print("âŒ Empty query after sanitization")
            raise HTTPException(status_code=400, detail="Query cannot be empty")
        
        if len(sanitized_query) > 5000:  # Reasonable limit
            print(f"âŒ Query too long: {len(sanitized_query)} characters")
            raise HTTPException(status_code=400, detail="Query is too long (max 5000 characters)")
        
        # Step 6: Execute query with comprehensive error handling
        print("ðŸ¤– Executing query with RAG system...")
        print(f"ðŸ” Query engine type: {type(query_engine)}")
        print(f"ðŸ” Sanitized query: '{sanitized_query[:100]}{'...' if len(sanitized_query) > 100 else ''}'")
        
        try:
            # Execute the actual query
            print("ðŸš€ Starting query execution...")
            response = query_engine.query(sanitized_query)
            print("âœ… Query executed successfully")
            print(f"ðŸ“ Response type: {type(response)}")
            
            # Extract response and source information
            response_text = str(response)
            print(f"ðŸ“ Response length: {len(response_text)} characters")
            
            source_count = 0
            if hasattr(response, 'source_nodes'):
                if response.source_nodes:
                    source_count = len(response.source_nodes)
                    print(f"ðŸ“„ Found {source_count} source nodes")
                else:
                    print("ðŸ“„ Source nodes list is empty")
            else:
                print("ðŸ“„ No source_nodes attribute found in response")
            
            # Check for empty response
            if not response_text or response_text.strip() == "":
                print("âš ï¸ Empty response generated")
                response_text = "I apologize, but I couldn't generate a meaningful response to your query. Please try rephrasing your question or check if the document contains relevant information."
            
            return QueryResponse(
                query=query_request.query,
                response=response_text,
                source_count=source_count,
            )
            
        except Exception as query_error:
            print(f"âŒ Query execution error: {type(query_error).__name__}")
            print(f"âŒ Query error details: {str(query_error)}")
            
            # Import traceback for detailed error information
            import traceback
            print(f"âŒ Query error traceback:")
            traceback.print_exc()
            
            # Classify different types of errors and provide specific handling
            error_str = str(query_error).lower()
            
            if "assertionerror" in error_str:
                print("âŒ Vector store assertion error detected - clearing system")
                rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
                raise HTTPException(
                    status_code=500,
                    detail="Document index error. Please re-upload your document."
                )
            elif "api" in error_str and ("key" in error_str or "token" in error_str):
                print("âŒ API Key error detected")
                raise HTTPException(
                    status_code=500, 
                    detail="AI model API error. Please check API keys and try again."
                )
            elif "openai" in error_str and "rate" in error_str:
                print("âŒ Rate limit error detected")
                raise HTTPException(
                    status_code=429,
                    detail="AI service rate limit exceeded. Please wait a moment and try again."
                )
            elif "embedding" in error_str:
                print("âŒ Embedding error detected")
                raise HTTPException(
                    status_code=500,
                    detail="Embedding model error. Please try again or contact support."
                )
            elif "connection" in error_str or "timeout" in error_str:
                print("âŒ Connection error detected")
                raise HTTPException(
                    status_code=500,
                    detail="Connection error to AI services. Please try again."
                )
            elif "index" in error_str or "retriever" in error_str:
                print("âŒ Index/Retriever error detected")
                rag_manager.clear_current_system(user_id=user_id, session_id=session_id)
                raise HTTPException(
                    status_code=500,
                    detail="Document index error. Please try re-uploading your document."
                )
            elif "memory" in error_str or "out of memory" in error_str:
                print("âŒ Memory error detected")
                raise HTTPException(
                    status_code=500,
                    detail="Server memory error. Please try with a shorter query or contact support."
                )
            else:
                print("âŒ Unknown error type")
                # For unknown errors, provide a sanitized error message
                error_message = str(query_error)
                if len(error_message) > 200:
                    error_message = error_message[:200] + "..."
                
                raise HTTPException(
                    status_code=500,
                    detail=f"Query processing error: {error_message}"
                )
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        print("ðŸ”„ Re-raising HTTPException")
        raise
    except Exception as e:
        print(f"âŒ Unexpected error in query endpoint: {type(e).__name__}: {str(e)}")
        
        # Import traceback for detailed error information
        import traceback
        print(f"âŒ Unexpected error traceback:")
        traceback.print_exc()
        
        # Generic error handling
        error_message = str(e)
        if len(error_message) > 200:
            error_message = error_message[:200] + "..."
            
        raise HTTPException(
            status_code=500, 
            detail=f"Internal server error: {error_message}"
        )

@app.get("/optimization-stats")
async def get_optimization_stats():
    """Get statistics about the optimization improvements."""
    model_cache_status = "unknown"
    if OPTIMIZED_SYSTEM_AVAILABLE:
        try:
            model_cache_status = "warmed" if model_manager._is_initialized else "cold"
        except:
            model_cache_status = "unavailable"
    
    return {
        "optimizations_applied": [
            "Singleton model manager (eliminates 3x LLM re-initialization)",
            "Rule-based filename extraction (no LLM calls)",
            "Reduced query expansions (3â†’1)",
            "Disabled expensive logical chunking", 
            "Model caching and reuse",
            "Background processing",
            "Fast regex-based pattern matching"
        ],
        "performance_improvements": {
            "before": {
                "typical_time_seconds": 300,
                "typical_time_minutes": 5,
                "llm_initializations": 3,
                "filename_method": "LLM-based with RAG queries",
                "query_expansions": 3,
            },
            "after": {
                "typical_time_seconds": 20,
                "typical_time_minutes": 0.33,
                "llm_initializations": 1,
                "filename_method": "Rule-based regex patterns",
                "query_expansions": 1,
            },
            "speedup_factor": "15x faster",
            "time_reduction": "95% reduction"
        },
        "model_cache_status": model_cache_status,
        "system_status": {
            "optimized_system_available": OPTIMIZED_SYSTEM_AVAILABLE,
            "security_available": SECURITY_AVAILABLE,
            "utils_available": UTILS_AVAILABLE
        },
        "config_optimizations": {
            "fine_chunk_size": 512,
            "retrieval_top_k": 50,
            "num_query_expansions": 1,
            "enable_logical_chunking": False,
            "enable_hybrid_retrieval": True
        }
    }

async def stream_query_document(request: Request, query_request: QueryRequest = None):
    """Stream query responses to prevent timeouts."""
    stream_start = time.time()
    print(f"ðŸŒŠ STREAM_QUERY_DOCUMENT STARTED at {stream_start}")

    from rag_pipeline.streaming_query_engine import create_streaming_engine

    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)
    print(f"ðŸ‘¤ USER: {user_id or 'anonymous'}, SESSION: {session_id}")

    # If query_request is None, parse it from the request body
    if query_request is None:
        try:
            body = await request.json()
            query_request = QueryRequest(**body)
        except Exception as e:
            print(f"âŒ Failed to parse request body: {e}")
            raise HTTPException(status_code=400, detail="Invalid request body")

    print(f"ðŸ” Streaming query received: '{query_request.query}'")
    print(f"ðŸ” Query length: {len(query_request.query)} characters")

    # Early detection: Check if this is a non-document query (greeting, etc.)
    def is_non_document_query(query: str) -> bool:
        """Quick check for greetings and general questions."""
        query_lower = query.lower().strip()
        non_doc_patterns = [
            'hi', 'hello', 'hey', 'good morning', 'good afternoon', 'good evening',
            'how are you', 'what\'s up', 'whats up', 'sup',
            'thank you', 'thanks', 'bye', 'goodbye',
            'who are you', 'what are you', 'what can you do',
        ]

        # Very short queries without question marks
        if len(query_lower) < 10 and '?' not in query_lower:
            for pattern in non_doc_patterns:
                if query_lower == pattern or query_lower.startswith(pattern + ' ') or query_lower == pattern + '?':
                    return True
        return False

    # If non-document query, respond immediately without loading RAG system
    if is_non_document_query(query_request.query):
        print(f"ðŸ’¬ Non-document query detected early, fast-track response")

        async def simple_response_stream():
            """Generate a simple response without document context."""
            responses = {
                'hi': "Hello! How can I help you with your document today?",
                'hello': "Hi there! How can I assist you?",
                'hey': "Hey! What can I help you with?",
                'how are you': "I'm doing well, thank you! How can I assist you with your document?",
                'thanks': "You're welcome! Let me know if you need anything else.",
                'thank you': "You're very welcome! Feel free to ask if you have more questions.",
            }

            query_lower = query_request.query.lower().strip()

            # Find matching response
            response_text = "Hello! I'm here to help you analyze your documents. Feel free to ask me anything about the uploaded document."
            for key, value in responses.items():
                if query_lower == key or query_lower.startswith(key + ' ') or query_lower == key + '?':
                    response_text = value
                    break

            # Send start event
            yield f"data: {json.dumps({'type': 'start', 'timestamp': time.time()})}\n\n"

            # Send response as chunks (simulate streaming)
            words = response_text.split()
            partial_response = ""
            for i, word in enumerate(words):
                chunk = word + (' ' if i < len(words) - 1 else '')
                partial_response += chunk
                yield f"data: {json.dumps({'type': 'content_chunk', 'chunk': chunk, 'partial_response': partial_response})}\n\n"
                await asyncio.sleep(0.01)  # Small delay for natural feel

            # Send end event
            yield f"data: {json.dumps({'type': 'stream_end', 'timestamp': time.time()})}\n\n"
            yield f"data: {json.dumps({'type': 'end', 'timestamp': time.time()})}\n\n"

        return StreamingResponse(
            simple_response_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )

    # Check if document is uploaded but RAG still building
    current_doc_id = None
    if user_id and user_id in rag_manager.user_sessions:
        current_doc_id = rag_manager.user_sessions[user_id]
    elif session_id and session_id in rag_manager.session_systems:
        current_doc_id = rag_manager.session_systems[session_id]

    if current_doc_id and OPTIMIZED_SYSTEM_AVAILABLE:
        rag_status = BackgroundRAGTracker.get_status(current_doc_id)
        if rag_status.get("status") == "processing":
            stage = rag_status.get("stage", "initializing")
            elapsed = time.time() - rag_status.get("started_at", time.time())
            eta = max(5, 60 - elapsed)

            print(f"â³ Document {current_doc_id} still indexing: {stage}")

            # Return streaming response with processing message
            async def processing_stream():
                message = f"â³ Your document is still being indexed (stage: {stage}). Please wait about {eta:.0f} more seconds and try again."
                yield f"data: {json.dumps({'type': 'content', 'content': message})}\n\n"
                yield f"data: {json.dumps({'type': 'done'})}\n\n"

            return StreamingResponse(
                processing_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )

    # Debug RAG manager state
    manager_status = rag_manager.get_status(user_id=user_id, session_id=session_id)
    print(f"ðŸ“Š RAG Manager Status: {manager_status}")
    print(f"ðŸ“Š Available systems: {list(rag_manager.systems.keys())}")
    print(f"ðŸ“Š User sessions: {rag_manager.user_sessions}")
    print(f"ðŸ“Š Session systems: {rag_manager.session_systems}")

    # Get current RAG system with fallback logic
    rag_system = rag_manager.get_current_system(user_id=user_id, session_id=session_id)
    print(f"ðŸ” RAG System found: {rag_system is not None}")

    # If no system found and we have a user_id, try to get any system for this user
    if not rag_system and user_id and user_id in rag_manager.user_sessions:
        document_id = rag_manager.user_sessions[user_id]
        if document_id in rag_manager.systems:
            rag_system = rag_manager.systems[document_id]["rag_system"]
            print(f"ðŸ”„ Using user's document: {document_id}")

    # If still no system, try the most recent system if available
    if not rag_system and rag_manager.systems:
        # Get the most recently used system
        recent_doc_id = max(rag_manager.systems.keys(),
                           key=lambda k: rag_manager.systems[k]["created_at"])
        rag_system = rag_manager.systems[recent_doc_id]["rag_system"]
        print(f"ðŸ”„ Using most recent document: {recent_doc_id}")

    if not rag_system:
        print(f"âŒ No RAG system available after fallback attempts")
        raise HTTPException(
            status_code=400,
            detail="No document has been uploaded and indexed yet. Please upload a document first."
        )
    
    # Get query engine and LLM
    query_engine = rag_system.get("query_engine")
    if not query_engine:
        raise HTTPException(
            status_code=500, 
            detail="Query engine not properly initialized. Please re-upload your document."
        )
    
    # Get LLM from embedding manager
    llm = None
    if "embedding_manager" in rag_system:
        embedding_manager = rag_system["embedding_manager"]
        if hasattr(embedding_manager, 'get_llm'):
            llm = embedding_manager.get_llm()

    if not llm:
        raise HTTPException(
            status_code=500,
            detail="LLM not available. Please re-upload your document."
        )

    # Get additional parameters for adaptive multi-question processing
    vector_index = rag_system.get("vector_index")
    nodes = rag_system.get("nodes")
    embedding_manager = rag_system.get("embedding_manager")
    total_pages = rag_system.get("total_pages", 0)

    # Debug: verify parameters are being extracted
    print(f"ðŸ” DEBUG: Extracted parameters for streaming engine:")
    print(f"   - vector_index: {vector_index is not None}")
    print(f"   - nodes: {nodes is not None} (count: {len(nodes) if nodes else 0})")
    print(f"   - embedding_manager: {embedding_manager is not None}")
    print(f"   - total_pages: {total_pages}")

    # Create streaming engine with all parameters for adaptive processing
    streaming_engine = create_streaming_engine(
        query_engine,
        llm,
        vector_index=vector_index,
        nodes=nodes,
        embedding_manager=embedding_manager,
        total_pages=total_pages
    )
    
    # Wrap engine with an immediate prelude to flush headers and show instant UI feedback
    async def sse_wrapper():
        # Immediate 'start' event
        start_event = {"type": "start", "timestamp": time.time(), "user_id": user_id or "anonymous"}
        yield f"data: {json.dumps(start_event)}\n\n"

        # Tiny first chunk so the client can render a bubble instantly
        priming_chunk = {"type": "content_chunk", "chunk": " "}
        yield f"data: {json.dumps(priming_chunk)}\n\n"

        # Delegate to the engine with voice mode flag
        async for event in streaming_engine.stream_query(
            query_request.query, 
            user_id or "anonymous",
            voice_mode=query_request.voice_mode
        ):
            # Directly forward engine events (already formatted as SSE lines)
            yield event

        # Final end event in case engine doesn't send one
        yield f"data: {json.dumps({'type': 'end', 'timestamp': time.time()})}\n\n"

    # Return streaming response with no buffering
    return StreamingResponse(
        sse_wrapper(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control",
            "X-Accel-Buffering": "no"
        }
    )

@app.get("/current-document")
async def get_current_document(request: Request):
    """Get information about the currently active document in RAG system."""
    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)
    
    status = rag_manager.get_status(user_id=user_id, session_id=session_id)
    current_file_path = rag_manager.get_current_file_path(user_id=user_id, session_id=session_id)
    current_doc_id = rag_manager.get_current_document_id(user_id=user_id, session_id=session_id)
    
    if status["has_current_system"] and current_doc_id:
        return {
            "has_document": True,
            "document_id": current_doc_id,
            "filename": os.path.basename(current_file_path) if current_file_path else None,
            "system_ready": True,
        }
    else:
        return {
            "has_document": False,
            "document_id": None,
            "filename": None,
            "system_ready": False,
        }

@app.get("/compare-naming-methods")
async def compare_naming_methods():
    """Compare old vs new naming methods."""
    return {
        "old_method": {
            "name": "LLM-based with RAG queries",
            "steps": [
                "Build temporary RAG system",
                "Make 3 separate LLM queries",
                "Parse responses",
                "Combine results"
            ],
            "typical_time": "10-30 seconds",
            "llm_calls": 3,
            "accuracy": "95%",
            "cost": "High (API calls)",
            "rate_limiting": "Yes, can hit limits"
        },
        "new_method": {
            "name": "Rule-based regex extraction",
            "steps": [
                "Extract first 3 pages text",
                "Apply regex patterns",
                "Match document types",
                "Build filename"
            ],
            "typical_time": "0.01-0.1 seconds",
            "llm_calls": 0,
            "accuracy": "90%",
            "cost": "Free (no API calls)",
            "rate_limiting": "No limits"
        },
        "speedup": "100-1000x faster for naming",
        "recommendation": "Use rule-based for most cases, LLM for complex edge cases"
    }

@app.delete("/reset")
async def reset_system():
    """Reset the RAG system with proper cleanup."""
    rag_manager.reset_all()
    return {
        "message": "All RAG systems reset successfully", 
        "note": "Models remain cached for speed"
    }

@app.get("/security-report")
async def get_security_report(request: Request):
    """Get security report for the current user."""
    user_id, _ = extract_user_id_from_token(request)
    
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    if not security:
        raise HTTPException(status_code=503, detail="Security middleware not available")
    
    try:
        report = security.get_security_report(user_id)
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to generate security report: {str(e)}")

@app.get("/health")
async def health_check(request: Request):
    """Health check endpoint for Railway deployment."""
    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)
    
    manager_status = rag_manager.get_status(user_id=user_id, session_id=session_id)
    current_file_path = rag_manager.get_current_file_path(user_id=user_id, session_id=session_id)
    
    # Safe model cache status check
    model_cache_status = "cold"
    if OPTIMIZED_SYSTEM_AVAILABLE:
        try:
            model_cache_status = "warmed" if model_manager._is_initialized else "cold"
        except:
            model_cache_status = "unknown"
    
    return {
        "status": "healthy",
        "system_ready": manager_status["has_current_system"],
        "pdf_loaded": current_file_path is not None,
        "models_cached": model_cache_status,
        "optimization_mode": "ultra_fast" if OPTIMIZED_SYSTEM_AVAILABLE else "fallback",
        "dependencies": {
            "optimized_system_available": OPTIMIZED_SYSTEM_AVAILABLE,
            "security_available": SECURITY_AVAILABLE,
            "utils_available": UTILS_AVAILABLE
        }
    }

@app.get("/performance-tips")
async def get_performance_tips():
    """Get tips for maximum performance."""
    return {
        "tips": [
            "Use the /upload-pdf-ultra-fast endpoint for maximum speed",
            "Keep the server running to maintain model cache",
            "Use rule-based naming for 90%+ accuracy with 1000x speed",
            "Upload PDF files directly when possible (avoid DOCX conversion)", 
            "Consider pre-processing documents to remove unnecessary pages",
            "Use smaller documents when possible (under 10MB optimal)"
        ],
        "benchmark_expectations": {
            "small_pdf_5_pages": "10-15 seconds",
            "medium_pdf_20_pages": "20-30 seconds", 
            "large_pdf_50_pages": "45-60 seconds",
            "very_large_pdf_100_pages": "90-120 seconds"
        },
        "bottlenecks_remaining": [
            "Vector embedding generation (GPU would help)",
            "Text extraction from complex PDFs",
            "Disk I/O for large files"
        ]
    }

# ================================
# COMPATIBILITY ENDPOINTS
# ================================

@app.post("/upload-pdf")
async def upload_pdf_compatibility(
    request: Request, 
    file: UploadFile = File(...),
    naming_option: str = Form('keep_original'),
    title: str = Form('Document'),
    client_name: str = Form('Client'),
    document_id: Optional[str] = Form(None)
):
    """
    Compatibility endpoint for /upload-pdf - redirects to ultra-fast endpoint
    """
    print(f"ðŸ”„ Compatibility endpoint called: /upload-pdf for {file.filename}")
    
    # Redirect to the ultra-fast endpoint with all parameters
    return await upload_document_ultra_fast(request, file, naming_option, title, client_name, document_id)


@app.get("/check-document/{document_id}")
async def check_document_exists(request: Request, document_id: str):
    """
    Check if a document exists in the RAG system
    """
    print(f"ðŸ” Checking document existence: {document_id}")
    
    user_id, _ = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)

    # ðŸ”¥ SIMPLIFIED: RAG system now uses database cuid IDs directly
    manager_status = rag_manager.get_status(user_id=user_id, session_id=session_id)
    current_file_path = rag_manager.get_current_file_path(user_id=user_id, session_id=session_id)
    
    # Check if this specific document exists
    document_exists = document_id in rag_manager.systems
    
    # If no specific document, check if any document is loaded
    if not document_exists and manager_status["has_current_system"]:
        document_exists = True
        if user_id and user_id in rag_manager.user_sessions:
            document_id = rag_manager.user_sessions[user_id]
        elif session_id and session_id in rag_manager.session_systems:
            document_id = rag_manager.session_systems[session_id]
    
    if document_exists and current_file_path:
        return {
            "exists": True,
            "document_id": document_id,
            "rag_id": document_id,  # Same as document_id now
            "filename": os.path.basename(current_file_path),
            "current_system_id": document_id
        }
    else:
        return {
            "exists": False,
            "document_id": document_id,
            "rag_id": None,
            "filename": None,
            "current_system_id": None
        }

@app.post("/activate-document/{document_id}")
async def activate_document_for_session(request: Request, document_id: str):
    """Activate a document for the current user/session, re-uploading to RAG system if necessary."""
    user_id, auth_token = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)

    print(f"ðŸ”„ Attempting to activate document: {document_id}")
    print(f"ðŸ“‹ Available documents in RAG system: {list(rag_manager.systems.keys())}")

    # ðŸ”¥ FIXED: If document exists in RAG system, just activate it
    if document_id in rag_manager.systems:
        print(f"âœ… Document {document_id} found in RAG system, activating...")
        
        # Point the user's/session's current mapping to this document
        if user_id:
            rag_manager.user_sessions[user_id] = document_id
            print(f"âœ… Activated existing document {document_id} for user {user_id}")
        elif session_id:
            rag_manager.session_systems[session_id] = document_id
            print(f"âœ… Activated existing document {document_id} for session {session_id}")

        return {
            "message": "Document activated for current session",
            "document_id": document_id,
            "status": "activated_existing"
        }

    # ðŸ”¥ OPTIMIZED: Check if file already exists locally before fetching from database
    print(f"ðŸ“¤ Document {document_id} not in RAG system, checking local files...")

    # Check if we have a local copy from previous upload
    # Files are named like: YYYYMMDD_DOCTYPE_CLIENT_#.pdf
    # But we stored the document_id in the upload process, so check all PDFs
    import glob

    # First, try to find by document_id in filename (if stored during upload)
    sample_docs_pattern = os.path.join("sample_docs", "*.pdf")
    all_local_files = glob.glob(sample_docs_pattern)

    # Match by checking which file was most recently associated with this document_id
    # For now, check all files and use the most recent one
    local_files = []

    # Try to find exact match first by checking RAG manager's systems dict
    if document_id in rag_manager.systems:
        stored_path = rag_manager.systems[document_id].get('file_path')
        if stored_path and os.path.exists(stored_path):
            local_files = [stored_path]
            print(f"âœ… Found file from RAG manager: {stored_path}")

    # If not found in systems, file must have been cleared from memory
    # In this case, we need to fetch from database (no local copy available)
    if not local_files:
        print(f"âš ï¸ No local file found for document {document_id}")
        local_files = []

    if local_files:
        # Found local file! Just rebuild RAG without re-fetching or renaming
        local_file_path = local_files[0]
        print(f"âœ… Found local file: {local_file_path}")
        print(f"ðŸš€ Fast reactivation: Building RAG only (skip download & naming)")

        try:
            if not OPTIMIZED_SYSTEM_AVAILABLE:
                raise HTTPException(status_code=503, detail="RAG system not available")

            # Extract text and build RAG directly (no renaming needed!)
            from utils.file_handler import extract_text_from_pdf
            documents = extract_text_from_pdf(local_file_path)

            if not documents:
                raise Exception("No text extracted from local file")

            print(f"ðŸ“„ Extracted {len(documents)} pages from local file")

            # Build RAG system directly
            from optimized_rag_system import VectorizedRAGBuilder
            rag_system = await VectorizedRAGBuilder.build_rag_system_fast(documents, local_file_path)

            if not rag_system or "query_engine" not in rag_system:
                raise Exception("RAG system incomplete")

            # Register with RAG manager
            rag_manager.set_system(
                document_id=document_id,
                rag_system=rag_system,
                file_path=local_file_path,
                user_id=user_id,
                session_id=session_id
            )

            print(f"âœ… Document {document_id} reactivated from local file (fast path)")

            return {
                "message": "Document reactivated from local file",
                "document_id": document_id,
                "status": "reactivated_from_local",
                "filename": os.path.basename(local_file_path)
            }

        except Exception as e:
            print(f"âŒ Fast reactivation failed: {e}, falling back to full re-upload")
            # Continue to full re-upload below

    # If no local file, fetch from database and do full re-upload
    print(f"ðŸ“¥ No local file found, fetching from database...")

    if not auth_token:
        raise HTTPException(status_code=401, detail="Authentication required to fetch document")

    try:
        # Fetch document from database via Next.js API
        import aiohttp
        async with aiohttp.ClientSession() as session:
            # Get document metadata
            # Use localhost in dev, NEXT_PUBLIC_APP_URL in production
            base_url = f"{os.environ.get('NEXT_PUBLIC_APP_URL', 'http://localhost:3000')}"
            doc_response = await session.get(
                f"{base_url}/backend/api/documents/{document_id}",
                headers={"Authorization": f"Bearer {auth_token}"}
            )

            if doc_response.status != 200:
                raise HTTPException(
                    status_code=404,
                    detail=f"Document {document_id} not found in database"
                )

            doc_data = await doc_response.json()
            print(f"ðŸ“„ Found document in database: {doc_data.get('originalFileName', 'Unknown')}")

            # Get document file
            file_response = await session.get(
                f"{base_url}/backend/api/documents/{document_id}/file",
                headers={"Authorization": f"Bearer {auth_token}"}
            )

            if file_response.status != 200:
                raise HTTPException(
                    status_code=404,
                    detail=f"Document file {document_id} not found or not accessible"
                )

            file_content = await file_response.read()
            filename = doc_data.get('originalFileName', f'document_{document_id}.pdf')

            print(f"ðŸ“ Retrieved file: {filename} ({len(file_content)} bytes)")

            # Create temporary file for RAG processing
            import tempfile

            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(file_content)
                temp_path = temp_file.name

            try:
                # Process document with RAG system
                if not OPTIMIZED_SYSTEM_AVAILABLE:
                    raise HTTPException(status_code=503, detail="RAG system not available")

                result = await optimized_upload_workflow(
                    file_content=file_content,
                    original_filename=filename,
                    naming_option='keep_original',
                    document_id=document_id,
                    rag_manager=rag_manager,
                    user_id=user_id,
                    session_id=session_id,
                    user_title='Document',
                    user_client_name='Client',
                    counter=1
                )
                
                if result and "rag_system" in result:
                    # Register with RAG manager
                    rag_manager.set_system(
                        document_id=document_id,
                        rag_system=result["rag_system"],
                        file_path=result["file_path"],
                        user_id=user_id,
                        session_id=session_id
                    )
                    
                    print(f"âœ… Document {document_id} re-uploaded and activated in RAG system")
                    
                    return {
                        "message": "Document re-uploaded and activated for current session",
                        "document_id": document_id,
                        "status": "re_uploaded_and_activated",
                        "filename": filename,
                        "processing_time": result.get("timing", {}).get("total", 0)
                    }
                else:
                    raise HTTPException(status_code=500, detail="Failed to process document with RAG system")
                    
            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_path)
                except:
                    pass
                    
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error re-uploading document: {str(e)}")
        raise HTTPException(
            status_code=500, 
            detail=f"Failed to re-upload document to RAG system: {str(e)}"
        )


@app.post("/reactivate-document/{document_id}")
async def reactivate_document_fast(request: Request, document_id: str):
    """Fast document reactivation without file renaming - for session loading"""
    import time
    start_time = time.time()
    
    user_id, auth_token = extract_user_id_from_token(request)
    session_id = extract_session_id_from_request(request)

    print(f"âš¡ FAST REACTIVATION: {document_id}")
    print(f"ðŸ‘¤ User: {user_id or 'anonymous'}, Session: {session_id}")

    # Step 1: Check if already in RAG memory
    if document_id in rag_manager.systems:
        print(f"âœ… Document already in memory, activating...")
        
        if user_id:
            rag_manager.user_sessions[user_id] = document_id
        elif session_id:
            rag_manager.session_systems[session_id] = document_id

        elapsed = time.time() - start_time
        return {
            "message": "Document activated from memory",
            "document_id": document_id,
            "status": "activated_from_memory",
            "processing_time": round(elapsed, 3)
        }

    # Step 2: Check if file exists locally (Railway may not have this)
    print(f"ðŸ“‚ Checking local files...")
    import glob
    sample_docs_pattern = os.path.join("sample_docs", "*.pdf")
    local_files = glob.glob(sample_docs_pattern)
    
    local_file_path = None
    for file_path in local_files:
        # Try to match by checking if this was the file for this document
        # This is a heuristic - in production you'd have a mapping
        if document_id in file_path or os.path.basename(file_path).startswith(document_id[:8]):
            local_file_path = file_path
            break
    
    if local_file_path and os.path.exists(local_file_path):
        print(f"âœ… Found local file: {local_file_path}")
        print(f"ðŸš€ Fast reactivation: Building RAG from local file (skip download & rename)")

        try:
            if not OPTIMIZED_SYSTEM_AVAILABLE:
                raise HTTPException(status_code=503, detail="RAG system not available")

            # Extract text and build RAG directly
            from utils.file_handler import extract_text_from_pdf
            documents = extract_text_from_pdf(local_file_path)

            if not documents:
                raise Exception("No text extracted from local file")

            print(f"ðŸ“„ Extracted {len(documents)} pages")

            # Build RAG system directly
            from optimized_rag_system import VectorizedRAGBuilder
            rag_system = await VectorizedRAGBuilder.build_rag_system_fast(documents, local_file_path)

            if not rag_system or "query_engine" not in rag_system:
                raise Exception("RAG system incomplete")

            # Register with RAG manager
            rag_manager.set_system(
                document_id=document_id,
                rag_system=rag_system,
                file_path=local_file_path,
                user_id=user_id,
                session_id=session_id
            )

            elapsed = time.time() - start_time
            print(f"âœ… Document reactivated from local file in {elapsed:.2f}s")

            return {
                "message": "Document reactivated from local file",
                "document_id": document_id,
                "status": "reactivated_from_local",
                "filename": os.path.basename(local_file_path),
                "processing_time": round(elapsed, 3)
            }

        except Exception as e:
            print(f"âŒ Local reactivation failed: {e}")
            # Continue to database fetch below

    # Step 3: Fetch from database with already-renamed fileName (skip renaming!)
    print(f"ðŸ“¥ Fetching from database (Railway path)...")

    if not auth_token:
        raise HTTPException(status_code=401, detail="Authentication required")

    try:
        import aiohttp
        async with aiohttp.ClientSession() as session:
            # Get document metadata
            base_url = f"{os.environ.get('NEXT_PUBLIC_APP_URL', 'http://localhost:3000')}"
            
            print(f"ðŸŒ Fetching from: {base_url}/backend/api/documents/{document_id}")
            
            doc_response = await session.get(
                f"{base_url}/backend/api/documents/{document_id}",
                headers={"Authorization": f"Bearer {auth_token}"}
            )

            if doc_response.status != 200:
                raise HTTPException(
                    status_code=doc_response.status,
                    detail=f"Document not found in database"
                )

            doc_data = await doc_response.json()
            # Use fileName (already renamed) instead of originalFileName
            renamed_filename = doc_data.get('fileName', doc_data.get('originalFileName', 'document.pdf'))
            print(f"ðŸ“„ Document: {renamed_filename} (already renamed)")

            # Get document file
            file_response = await session.get(
                f"{base_url}/backend/api/documents/{document_id}/file",
                headers={"Authorization": f"Bearer {auth_token}"}
            )

            if file_response.status != 200:
                raise HTTPException(
                    status_code=file_response.status,
                    detail=f"Document file not accessible"
                )

            file_content = await file_response.read()
            print(f"ðŸ“ Retrieved file: {len(file_content)} bytes")

            # Save to sample_docs with the already-renamed filename
            os.makedirs("sample_docs", exist_ok=True)
            saved_file_path = os.path.join("sample_docs", renamed_filename)
            
            with open(saved_file_path, 'wb') as f:
                f.write(file_content)
            
            print(f"ðŸ’¾ Saved to: {saved_file_path}")

            # Build RAG system directly (NO RENAMING!)
            if not OPTIMIZED_SYSTEM_AVAILABLE:
                raise HTTPException(status_code=503, detail="RAG system not available")

            from utils.file_handler import extract_text_from_pdf
            documents = extract_text_from_pdf(saved_file_path)

            if not documents:
                raise Exception("No text extracted from file")

            print(f"ðŸ“„ Extracted {len(documents)} pages")

            # Build RAG system
            from optimized_rag_system import VectorizedRAGBuilder
            rag_system = await VectorizedRAGBuilder.build_rag_system_fast(documents, saved_file_path)

            if not rag_system or "query_engine" not in rag_system:
                raise Exception("RAG system incomplete")

            # Register with RAG manager
            rag_manager.set_system(
                document_id=document_id,
                rag_system=rag_system,
                file_path=saved_file_path,
                user_id=user_id,
                session_id=session_id
            )

            elapsed = time.time() - start_time
            print(f"âœ… Document reactivated from database in {elapsed:.2f}s (skipped renaming)")

            return {
                "message": "Document reactivated from database",
                "document_id": document_id,
                "status": "reactivated_from_database",
                "filename": renamed_filename,
                "processing_time": round(elapsed, 3),
                "note": "Used already-renamed file, skipped renaming process"
            }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Fast reactivation failed: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Failed to reactivate document: {str(e)}"
        )


# ================================
# MAIN FUNCTION - FIXED FOR RAILWAY
# ================================

def main():
    """Main function to run the ultra-fast RAG API with Railway support."""
    print("ðŸš€ Starting RAG Pipeline API with GPT-5...")
    
    # FIXED: Use Railway's PORT environment variable
    port = int(os.environ.get("PORT", 8000))
    print(f"ðŸŒ Starting server on port: {port}")
    print(f"ðŸ”§ Environment: {'Production' if port != 8000 else 'Development'}")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,  # FIXED: Use dynamic port from Railway
        reload=False,  # Disable reload for production performance
        log_level="info",
        workers=1  # Single worker to maintain model cache
    )

if __name__ == "__main__":
    main()